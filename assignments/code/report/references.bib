@article{gao2023retrieval,
  title={Retrieval-augmented generation for large language models: A survey},
  author={Gao, Yunfan and Xiong, Yun and Gao, Xinyu and Jia, Kangxiang and Pan, Jinliu and Bi, Yuxi and Dai, Yi and Sun, Jiawei and Wang, Meng and Wang, Haofen},
  journal={arXiv preprint arXiv:2312.10997},
  year={2023}
}

@article{chan2024rq,
  title={RQ-RAG: Learning to Refine Queries for Retrieval Augmented Generation},
  author={Chan, Chi-Min and Xu, Chunpu and Yuan, Ruibin and Luo, Hongyin and Xue, Wei and Guo, Yike and Fu, Jie},
  journal={arXiv preprint arXiv:2404.00610},
  year={2024}
}

@article{chang2025main,
  title={MAIN-RAG: Multi-Agent Filtering Retrieval-Augmented Generation},
  author={Chang, Chia-Yuan and Jiang, Zhimeng and Rakesh, Vineeth and Pan, Menghai and Yeh, Chin-Chia Michael and Wang, Guanchu and Hu, Mingzhi and Xu, Zhichao and Zheng, Yan and Das, Mahashweta and Zou, Na},
  journal={arXiv preprint arXiv:2501.00332},
  year={2025}
}

@article{jin2025hierarchical,
  title={Hierarchical Document Refinement for Long-context Retrieval-augmented Generation},
  author={Jin, Jiajie and Li, Xiaoxi and Dong, Guanting and Zhang, Yuyao and Zhu, Yutao and Wu, Yongkang and Li, Zhonghua and Ye, Qi and Dou, Zhicheng},
  journal={arXiv preprint arXiv:2505.10413},
  year={2025}
}

@article{xu2020layoutlm,
  title={LayoutLM: Pre-training of Text and Layout for Document Image Understanding},
  author={Xu, Yiheng and Li, Minghao and Cui, Lei and Huang, Shaohan and Wei, Furu and Zhou, Ming},
  journal={Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery \& Data Mining},
  year={2020}
}

@article{xu2021layoutlmv2,
  title={LayoutLMv2: Multi-modal Pre-training for Visually-Rich Document Understanding},
  author={Xu, Yang and Xu, Yiheng and Lv, Tengchao and Cui, Lei and Wei, Furu and Wang, Guoxin and Lu, Yijuan and Florencio, Dinei and Zhang, Cha and Che, Wanxiang and Zhang, Min and Zhou, Lidong},
  journal={Proceedings of the 59th Annual Meeting of the Association for Computational Linguistics},
  year={2021}
}

@article{huang2022layoutlmv3,
  title={LayoutLMv3: Pre-training for Document AI with Unified Text and Image Masking},
  author={Huang, Yupan and Lv, Tengchao and Cui, Lei and Lu, Yijuan and Wei, Furu},
  journal={Proceedings of the 30th ACM International Conference on Multimedia},
  year={2022}
}

@article{kim2021donut,
  title={OCR-free Document Understanding Transformer},
  author={Kim, Geewook and Hong, Teakgyu and Yim, Moonbin and Nam, JeongYeon and Park, Jinyoung and Yim, Jinyeong and Hwang, Wonseok and Yun, Sangdoo and Han, Dongyoon and Park, Seunghyun},
  journal={European Conference on Computer Vision},
  year={2022}
}

@article{blecher2023nougat,
  title={Nougat: Neural Optical Understanding for Academic Documents},
  author={Blecher, Lukas and Cucurull, Guillem and Scialom, Thomas and Stojnic, Robert},
  journal={arXiv preprint arXiv:2308.13418},
  year={2023}
}

@article{tang2022udop,
  title={Unifying Vision, Text, and Layout for Universal Document Processing},
  author={Tang, Zineng and Yang, Ziyi and Wang, Guoxin and Fang, Yuwei and Liu, Yang and Zhu, Chenguang and Zeng, Michael and Zhang, Cha and Bansal, Mohit},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2023}
}

@article{smith2007tesseract,
  title={An Overview of the Tesseract OCR Engine},
  author={Smith, Ray},
  journal={Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)},
  year={2007}
}

@article{du2020pp-ocrv2,
  title={PP-OCRv2: Bag of Tricks for Ultra Lightweight OCR System},
  author={Du, Yuning and Li, Chenxia and Guo, Ruoyu and Yin, Xiaoting and Liu, Weiwei and Zhou, Jun and Bai, Yifan and Yu, Zilin and Yang, Yehua and Dang, Qingqing and Wang, Haoshuang},
  journal={arXiv preprint arXiv:2109.03144},
  year={2021}
}

@misc{llamaindex2024,
  title={LlamaParse: Advanced Document Parsing},
  author={LlamaIndex},
  url={https://github.com/run-llama/llama_parse},
  year={2024}
}

@article{zhang2024document,
  title={A Comprehensive Survey on Document Parsing: Challenges, Methods, and Prospects},
  author={Zhang, Pengfei and Li, Zhenrong and Cao, Jianshu and Zhao, Rui and Lu, Shengchao and Qiao, Yu and Zhao, Shuai},
  journal={arXiv preprint arXiv:2406.11906},
  year={2024}
}

@misc{procycons2024benchmark,
  title={Comprehensive Benchmark of PDF Parsing Tools: Docling vs. Unstructured vs. LlamaParse},
  author={Procycons},
  url={https://medium.com/@procycons/comprehensive-benchmark-of-pdf-parsing-tools-docling-vs-unstructured-vs-llamaparse-85bdbc1e456},
  year={2024}
}

@article{luo2023geolayoutlm,
  title={GeoLayoutLM: Geometric Pre-training for Visual Information Extraction},
  author={Luo, Chuwei and Jin, Lianwen and Sun, Zenghui},
  journal={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  year={2023}
}

@article{zhu2025laytokenllm,
  title={LayTokenLLM: A Simple yet Effective Layout Token Approach for Multi-modal Document Understanding},
  author={Zhu, Yutao and Dong, Guanting and Jin, Jiajie and Li, Xiaoxi and Zhang, Yuyao and Wu, Yongkang and Li, Zhonghua and Ye, Qi and Dou, Zhicheng},
  journal={arXiv preprint arXiv:2501.09324},
  year={2025}
}

@article{borchmann2021due,
  title={DUE: End-to-End Document Understanding Benchmark},
  author={Borchmann, Łukasz and Pietruszka, Michał and Stauffer, Tomasz and Jurkiewicz, Dawid and Szyndler, Aleksander and Gretkowski, Andrzej and Gawlik, Izabela and Garncarek, Łukasz and Powalski, Rafał and Jassem, Karol and Graliński, Filip},
  journal={Advances in Neural Information Processing Systems},
  year={2021}
}

@article{park2019cord,
  title={CORD: A Consolidated Receipt Dataset for Post-OCR Parsing},
  author={Park, Seunghyun and Shin, Seung and Lee, Bado and Lee, Junyeop and Surh, Jaeheung and Seo, Minjoon and Lee, Hwalsuk},
  journal={Workshop on Document Intelligence at NeurIPS 2019},
  year={2019}
}

@article{zhao2024comprehensive,
  title={A Comprehensive Review of Retrieval-Augmented Generation: Evolution, Current Landscape and Future Directions},
  author={Zhao, Zhicheng and Wang, Yiyang and Zhang, Cheng and Zhao, Shuai and Dou, Zhicheng},
  journal={arXiv preprint arXiv:2410.12837},
  year={2024}
}

@misc{zhuang2024pdftomarkdownllm,
  title={PDFToMarkdownLLM: A Pluggable Tool for LLM-Agent PDF Processing},
  author={Zhuang, Shanning},
  year={2024},
  note={Available at: \url{https://github.com/ShanningZhuang/PDFToMarkdownLLM}},
  url={https://github.com/ShanningZhuang/PDFToMarkdownLLM}
} 